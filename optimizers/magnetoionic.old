import torch
import numpy as np
from torch.nn.utils import parameters_to_vector, vector_to_parameters


class Magnetoionic(torch.optim.Optimizer):

    def __init__(self,
                 params,
                 lr=1e-3,
                 lower_bounds_minus=(-5, 35),
                 lower_bounds_plus=(-5, 100),
                 upper_bounds=(-1, 1),
                 step=1e-3,
                 field=False,
                 eps=1e-8):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 <= eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))

        defaults = dict(lr=lr,
                        eps=eps,
                        lower_bounds_minus=lower_bounds_minus,
                        lower_bounds_plus=lower_bounds_plus,
                        upper_bounds=upper_bounds)
        super(Magnetoionic, self).__init__(params, defaults)

        # Create the x axis for the polynomial approximation
        self.x_minus = torch.arange(
            lower_bounds_minus[0], lower_bounds_minus[1], step)
        self.x_plus = torch.arange(
            lower_bounds_plus[0], lower_bounds_plus[1], step)

        # Depending on the field applied, we have different polynomials
        if field:
            # -1.34778355e-07,  1.61122873e-05, -6.15441759e-04,  1.01673670e-02,-1.42614553e-01,  9.54573777e-01
            self.f_minus = lambda x: -1.34778355e-07*x**5 + 1.61122873e-05*x**4 - 6.15441759e-04 * \
                x**3 + 1.01673670e-02*x**2 - 1.42614553e-01*x + 9.54573777e-01
            # 3.29202772e-09, -9.72529648e-07,  1.07630427e-04, -5.54258723e-03,1.36555413e-01, -5.74642191e-01
            self.f_plus = lambda x: 3.29202772e-09*x**5 - 9.72529648e-07*x**4 + 1.07630427e-04 * \
                x**3 - 5.54258723e-03*x**2 + 1.36555413e-01*x - 5.74642191e-01
        else:
            # -2.48710727e-07,  2.67738280e-05, -1.08406929e-03,  2.16105443e-02,-2.57372086e-01,  8.96111091e-01
            self.f_minus = lambda x: -2.48710727e-07*x**5 + 2.67738280e-05*x**4 - 1.08406929e-03 * \
                x**3 + 2.16105443e-02*x**2 - 2.57372086e-01*x + 8.96111091e-01
            # 7.31733076e-10, -2.53820263e-07,  3.53910843e-05, -2.52637034e-03, 9.73265083e-02, -8.09844517e-01
            self.f_plus = lambda x: 7.31733076e-10*x**5 - 2.53820263e-07*x**4 + 3.53910843e-05 * \
                x**3 - 2.52637034e-03*x**2 + 9.73265083e-02*x - 8.09844517e-01

        # Create the y axis, and bound it if needed
        self.y_minus = self.f_minus(self.x_minus)
        self.y_minus = torch.where(self.y_minus > upper_bounds[1], upper_bounds[1]*torch.ones_like(
            self.y_minus), torch.where(self.y_minus < upper_bounds[0], upper_bounds[0]*torch.ones_like(self.y_minus), self.y_minus))

        self.y_plus = self.f_plus(self.x_plus)
        self.y_plus = torch.where(self.y_plus > upper_bounds[1], upper_bounds[1]*torch.ones_like(
            self.y_plus), torch.where(self.y_plus < upper_bounds[0], upper_bounds[0]*torch.ones_like(self.y_plus), self.y_plus))

    def __setstate__(self, state):
        super(Magnetoionic, self).__setstate__(state)
        for group in self.param_groups:
            group.setdefault('amsgrad', False)

    def step(self, closure=None):
        """Performs a single optimization step.

        Arguments:
            closure(callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for i, p in enumerate(group['params']):
                if p.grad is None:
                    continue
                grad = p.grad.data
                state = self.state[p]
                lower_bounds_minus = group['lower_bounds_minus']
                lower_bounds_plus = group['lower_bounds_plus']
                output_bounds = group['upper_bounds']
                lr = group['lr']

                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                state['step'] += 1
                # Flatten the parameters to iterate over them
                p_data = parameters_to_vector(p.data)
                grad = parameters_to_vector(grad)
                # Compute x = f^-1(w), the previous x of the old weights
                # For each weight, find the closest value of y_plus or y_minus in the polynom depending on the sign of the gradient.
                inverse = torch.tensor([self.x_plus[torch.argmin(
                    torch.abs(self.y_plus - w))] if g > 0 else self.x_minus[torch.argmin(torch.abs(self.y_minus - w))] for w, g in zip(p_data, grad)])
                # Compute x_t = x + grad, the next value of the parameters
                x_t = inverse + lr*grad
                print(inverse, lr*grad)

                x_t = torch.where(grad > 0, torch.where(x_t > lower_bounds_plus[1], lower_bounds_plus[1]*torch.ones_like(
                    x_t), torch.where(x_t < lower_bounds_plus[0], lower_bounds_plus[0]*torch.ones_like(x_t), x_t)), torch.where(x_t > lower_bounds_minus[1], lower_bounds_minus[1]*torch.ones_like(
                        x_t), torch.where(x_t < lower_bounds_minus[0], lower_bounds_minus[0]*torch.ones_like(x_t), x_t)))
                # Compute the value of the polynom at the new position to get the new weights w_t = x_t + grad
                w_t = torch.tensor(
                    [self.f_plus(x) if g > 0 else self.f_minus(x) for x, g in zip(x_t, grad)])
                # Bound the output value if needed
                w_t = torch.where(w_t > output_bounds[1], output_bounds[1]*torch.ones_like(
                    w_t), torch.where(w_t < output_bounds[0], output_bounds[0]*torch.ones_like(w_t), w_t))
                # Update the parameters
                vector_to_parameters(w_t, p.data)
        return loss
