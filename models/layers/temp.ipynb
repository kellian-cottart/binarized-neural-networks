{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernouilliWeights:\n",
    "    \"\"\" Bernouilli Weights\n",
    "\n",
    "    Represents bayesian weights with a bernouilli distribution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_):\n",
    "        \"\"\" Initialize the weights with a bernouilli distribution \n",
    "\n",
    "        Args:\n",
    "            lambda_ (torch.Tensor): Tensor of the same shape as the weights, represents the certainty of the weights of either being 1 or -1\n",
    "        \"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "        self.uniform = torch.distributions.uniform.Uniform(0, 1)\n",
    "\n",
    "    def sample(self, samples=1):\n",
    "        \"\"\" Sample from the exponential distribution using the Gumbel-softmax trick\"\"\"\n",
    "\n",
    "        # 1. Sample from the uniform distribution U(0, 1) the logistic noise (G1 - G2)\n",
    "        # Shape should be (samples, *self.lambda_.shape)\n",
    "        logistic_noise = self.uniform.sample(\n",
    "            (samples, *self.lambda_.shape)).to(self.lambda_.device)\n",
    "        # 2. Compute delta = 1/2 * log(U/(1-U))\n",
    "        # Shape should be (samples, *self.lambda_.shape)\n",
    "        delta = torch.log(logistic_noise / (1 - logistic_noise)) / 2\n",
    "        # 3. Compute the relaxed weights w.r.t the mean\n",
    "        # Shape should be (samples, *self.lambda_.shape)\n",
    "        relaxed_w = torch.tanh((self.lambda_ + delta))\n",
    "        return relaxed_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5204, -0.3785, -0.8682, -0.5991],\n",
      "         [ 0.5199, -0.1939,  0.9848,  0.5348],\n",
      "         [-0.5119,  0.7114, -0.5791,  0.0712],\n",
      "         [ 0.9896,  0.4623, -0.9462, -0.2486]],\n",
      "\n",
      "        [[-0.1238,  0.3015,  0.1561, -0.7636],\n",
      "         [-0.3629,  0.8665,  0.7653,  0.8692],\n",
      "         [ 0.2279,  0.2945, -0.1524, -0.9783],\n",
      "         [-0.0323,  0.2806, -0.9515, -0.8894]],\n",
      "\n",
      "        [[-0.6353, -0.9143,  0.3775, -0.0537],\n",
      "         [ 0.2229,  0.7806,  0.8320,  0.6712],\n",
      "         [-0.3936, -0.9235, -0.6569, -0.9744],\n",
      "         [-0.5836, -0.7403, -0.1723, -0.3913]],\n",
      "\n",
      "        [[ 0.7785, -0.8510, -0.1429, -0.2452],\n",
      "         [ 0.9694,  0.9219,  0.5260,  0.3916],\n",
      "         [ 0.5950, -0.6732,  0.9818, -0.5747],\n",
      "         [ 0.1652, -0.1687, -0.9570, -0.7221]],\n",
      "\n",
      "        [[ 0.0991, -0.3027,  0.9812, -0.8097],\n",
      "         [ 0.9450, -0.7345,  0.9936, -0.1031],\n",
      "         [-0.6236,  0.6347,  0.9114, -0.3396],\n",
      "         [-0.9018,  0.5795, -0.6098, -0.4076]]], grad_fn=<TanhBackward0>)\n",
      "tensor([[[-0.5204, -0.1238, -0.6353,  0.7785,  0.0991],\n",
      "         [ 0.5199, -0.3629,  0.2229,  0.9694,  0.9450],\n",
      "         [-0.5119,  0.2279, -0.3936,  0.5950, -0.6236],\n",
      "         [ 0.9896, -0.0323, -0.5836,  0.1652, -0.9018]],\n",
      "\n",
      "        [[-0.3785,  0.3015, -0.9143, -0.8510, -0.3027],\n",
      "         [-0.1939,  0.8665,  0.7806,  0.9219, -0.7345],\n",
      "         [ 0.7114,  0.2945, -0.9235, -0.6732,  0.6347],\n",
      "         [ 0.4623,  0.2806, -0.7403, -0.1687,  0.5795]],\n",
      "\n",
      "        [[-0.8682,  0.1561,  0.3775, -0.1429,  0.9812],\n",
      "         [ 0.9848,  0.7653,  0.8320,  0.5260,  0.9936],\n",
      "         [-0.5791, -0.1524, -0.6569,  0.9818,  0.9114],\n",
      "         [-0.9462, -0.9515, -0.1723, -0.9570, -0.6098]],\n",
      "\n",
      "        [[-0.5991, -0.7636, -0.0537, -0.2452, -0.8097],\n",
      "         [ 0.5348,  0.8692,  0.6712,  0.3916, -0.1031],\n",
      "         [ 0.0712, -0.9783, -0.9744, -0.5747, -0.3396],\n",
      "         [-0.2486, -0.8894, -0.3913, -0.7221, -0.4076]]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lambda_ = torch.nn.parameter.Parameter(torch.empty(\n",
    "            (4, 4)))\n",
    "lambda_.data = torch.distributions.normal.Normal(0, 1).sample(lambda_.shape)\n",
    "weights = BernouilliWeights(lambda_)\n",
    "relaxed_w = weights.sample(5)\n",
    "print(relaxed_w)\n",
    "print(relaxed_w.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binarized-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
